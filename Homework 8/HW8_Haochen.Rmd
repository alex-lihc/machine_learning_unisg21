---
title: |
  | Machine Learning, Spring 2021
  | Homework 8
author: "Daniil Bulat, Jonas Josef Huwyler, Haochen Li,Giovanni Magagnin"
date: "5/6/2021"
output:
  pdf_document: default
  html_document: default
---

```{r include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exercise 1
**
Consider the Iris data set, with only the first 100 data samples (setosa and versicolor), and only the features *sepal_length* and *petal_length*.
**

```{r echo = FALSE}
# Set up
rm(list = ls())
mainDir = "C:/Labs/Machine Learning/HW8"
setwd(mainDir)

# Package space 
# install.packages("e1071")
library(e1071)
library(ggplot2)
```

## Question 1
**
Run *svm* on the data (without scaling) with cost equal to 10,1,0.1, respectively, and plot the corresponding outcome. What difference do you see, and how are they related to the cost values.
**

```{r collapse = TRUE,results = 'hold'}
# set random seed
set.seed(123)
# Data Preparation
Data = iris[sample(1:100),c(1,3,5)]
# svm with cost = 0.1
fit1 = svm(Species ~., data= Data, kernel = "linear", cost = "0.1",scale = FALSE)
# svm with cost = 1
fit2 = svm(Species ~., data= Data, kernel = "linear", cost = "1",scale = FALSE)
# svm with cost = 10
fit3 = svm(Species ~., data= Data, kernel = "linear", cost = "10",scale = FALSE)
```

```{r out.width='90%', fig.show = 'hold', fig.align='center'}
# plot svm model 1
plot(fit1,data = Data)
mtext(side = 1, text = "cost = 0.1",cex = 2, outer = F)
# plot svm model 2
plot(fit2,data = Data)
mtext(side = 1, text = "cost = 1",cex = 2, outer = F)
# plot svm model 3
plot(fit3,data = Data)
mtext(side = 1, text = "cost = 10",cex = 2, outer = F)
```

The plots of SVM results are shown above. Here we keep the legends of three classes to compare results of part 4.

With different costs, the shape of separating line and the corresponding margins (can be observed from types of points) change.As the cost increase, the points on the margin or violating the margin (denoted by "x") decrease. In other words, as the cost increase, the number of support vectors becomes smaller, so the separating line defined by the support vectors changes and the margin decreases.


## Question 2
**
For *cost = 0.1* list the support vectors.
**

```{r}
# For cost = 0.1 list the support vectors.
print(fit1$SV)
```





## Question 3
**
For *cost = 0.1* find the equation of the separating line, as explained in Chapter 10 slide 17.
**

```{r results = 'hold'}
# For cost = 0.1 find the equation of separating line.
cat("The beta_0 is", - fit1$rho, '\n')
cat ("The coefficients are ",t(fit1$coefs)%*%fit1$SV)
```
Here we compute the corresponding coefficients as above. Thus, the equation of the separating line is
$$
2.442 + 0.02 x_1 -0.96x_2 = 0
$$


## Question 4
**
Now consider the data samples for versicolor and virginica, and run svm on the data (without scaling) with cost equal to 1. How many support vectors do you get and how does this compare to the result of part 1. (for the same cost)? Can you explain this behavior with respect to the different data sets (and the position of their points)?
**
```{r}
# Data Preparationg
set.seed(123)
Data2 = iris[sample(51:150),c(1,3,5)]
# SVM with cost = 1
fit4 = svm(Species ~., data= Data2, kernel = "linear", cost = "1",scale = FALSE)
# list the support vectors.
print(fit4$SV)
cat("The number of support vectors is", nrow(fit4$SV),"\n")
```

```{r out.width='90%', fig.show = 'hold', fig.align='center'}
# plot svm model 4
plot(fit4,data = Data2)
mtext(side = 1, text = "cost = 1",cex = 2, outer = F)
```
For the svm on the new data set, we have 31 support vectors, which is more than the case of svm result on the old data set with same cost, 4 support vectors.

A possible explanation could be that the new data set is more concentrated distribution. That is to say, under the condition of same cost, there are more points located in the wrong side of the margin.

\newpage
# Exercise 2
**
Consider the following trainign data, where the first half is in class -1 and the second half is in class 1:
$$\{(-1,2),(1,1),(0.5,3),(1,4),  (3,2),(3.5,1),(3.5,4),(4,2)\}$$
The maximal margin hyperplane is given by the equation $x_1 = 2$ (i.e, a vertical line intersecting the $x_1$-axis at the point 2).
**

```{r echo = FALSE}
# Data Preparation
Data.Ex2 = t(data.frame(c(-1,2),c(1,1),c(0.5,3),c(1,4),c(3,2),c(3.5,1),c(3.5,4),c(4,2)))
Data.Ex2 = data.frame(Data.Ex2)
colnames(Data.Ex2) = c("x","y")
rownames(Data.Ex2) = 1:nrow(Data.Ex2)
Data.Ex2["class"] = c(-1,-1,-1,-1,1,1,1,1)
Data.Ex2$class = as.factor(Data.Ex2$class)
```



## Question 1
**
Plot the hyperplane and the margin (on both sides). What are the support vectors and how large is the margin?
**

```{r echo = FALSE,out.width='90%', fig.show = 'hold', fig.align='center'}
# Plot the hyperplane and the margin.
fig1 = ggplot(Data.Ex2,aes(x,y, color = class)) + 
  geom_point() +
  geom_vline(xintercept = 2, col = "red") +
  geom_vline(xintercept = 1, col = "blue", lty = 2) +
  geom_vline(xintercept = 3, col = "blue", lty = 2)
# show the plot
print(fig1)
```
As we can find from the plot, the support vectors are $\{(1,1),(1,4),  (3,2)\}$. The margin is 1.

## Question 2
**
If we leave the separating hyperplane as it is, but increase the margin to 1.5, then what are the support vectors?
**

```{r echo = FALSE,out.width='90%', fig.show = 'hold', fig.align='center'}
# Plot the hyperplane and the soft margin.
fig2 = ggplot(Data.Ex2,aes(x,y, color = class)) + 
  geom_point() +
  geom_vline(xintercept = 2, col = "red") +
  geom_vline(xintercept = 0.5, col = "blue", lty = 2) +
  geom_vline(xintercept = 3.5, col = "blue", lty = 2)
# show the plot
print(fig2)
```
As we can find from the plot, the support vectors are $\{(1,1),(0.5,3),(1,4),  (3,2),(3.5,1),(3.5,4)\}$. 


\newpage
# Exercise 3
**
Plot the graphs of the following quadratic equations:
**

1. $\frac{x_1^2}{2}+\frac{x_2^2}{4} = 1$

```{r, echo=FALSE,out.width='50%',  fig.align='center'}
knitr::include_graphics("ex3fig1.png")
```

2. $\frac{x_1^2}{2}-\frac{x_2^2}{1} = 2$

```{r, echo=FALSE,out.width='50%',  fig.align='center'}
knitr::include_graphics("ex3fig2.png")
```

3. $x_2 + x_1^2= 2$

```{r, echo=FALSE,out.width='50%',  fig.align='center'}
knitr::include_graphics("ex3fig3.png")
```

4. $x_1-x_2+x_1x_2$

```{r, echo=FALSE,out.width='50%',  fig.align='center'}
knitr::include_graphics("ex3fig4.png")
```